Genome-scale metabolic models have been widely used to reconstruct bacterial organisms  \textit{in silico} and to simulate their phenotype in different media. The end goal is to accumulate as much information as possible to obtain models that resemble the actual organisms and are entirely predictive of their capacity. Genome-scale metabolic models are knowledge-bases of metabolism and as such they are built in a hypothesis-free manner and used to interrogate the model for a set of questions. ODE-based dynamical models are smaller in coverage of biological process although they harbour crucial dynamical properties that can further characterize the organism in time and space. The dynamical models are poorly scalable and are usually hypothesis-driven, which means that they are built in order to answer a specific question at a specific time. As I detailed in the introduction, building hybrid models of metabolism using both ODE-based models and  steady-state genome-scale models, can leverage the advantages of both types of models to increase the accuracy and scope of predictions. Although, a question that hasn't been brought up enough in the community surrounds the compatibility of these modeling techniques. Is the hybrid model hypothesis-free or hypothesis-driven? Is the model valid for all time, in all conditions or does it live for a short period of time to answer a specific question? What is the compromise in time interval for a biological system to achieve steady-state and meet the integration tolerance of ODEs? Is the model the smallest set of equations that better describe the data? Do the steady-state constraints apply in short-time dynamics?.\\
Answering these questions is crucial to properly assess the range of action of hybrid models. 
%For example, using economical models outside their application conditions drove austerity policies in countries that did not apply, thereby affecting the lives of millions of people \cite{leigh2013growth}, let alone biomedical and clinical trial models.
\section{Constraining dynamical models, how much is enough?}
Coupling dynamical ODE-based models and genome-scale metabolic models, requires points of intersection in metabolites or reactions. Oftentimes, the intersection set is a small set in comparison to the size of the metabolic model. Then we can legitimately ask if the changes induced by the intersecting set are enough to induce a whole-system shift in metabolism and to drive a dynamic change in phenotype, dictated by the dynamical model. In the first coupled model of Ecoli \cite{covert2008integrating}, the authors coupled all available reactions and metabolites in the dynamical model to the genome-scale model. In return the genome-scale model informed the ODE model with new rates specific to the biomass. The coupled model provided closer results to the experimental data than each of the model on their own. Later in the first model combining PBPK and genome-scale models \cite{krauss2012integrating}, the intersection points were only two reactions, namely the organ perfusion and the organ secretion for allopurinol, which matched the uptake and secretion reactions in the genome-scale model. We used a similar approach to predict the pharmaockinetics of levodopa with a set of diets (\hyperref[ch:chapter3]{Chapter 3}). Two matching reactions here are enough to predict the concentrations of allopurinol, because in PBPK models, the concentrations are entirely determined by in and out rates. In another work \cite{tummler2015dynamic}, a dynamical model has been embedded into a genome-scale model, to backtrack the contribution of metabolites to the biomass, which allowed to correct the predictions of the dynamical model using species that were not initially included in the ODEs.\\
So to the question 'How many dynamical constraints should I apply to the genome-scale model?' my answer would vary with the question sought. It stays obvious that a large number of constraints involving key processes like the biomass and imbalanced reactions are drivers of metabolic shifts in large-scale metabolic models. If the question surrounds the prediction of the concentration of a specific set of metabolites then coupling reactions directly involved in their anabolic and catabolic processes is sufficient, independently of the original size of the model.\\
Equally, achieving steady state is a central assumption in genome-scale metabolic models and forms the basis for Flux Balance Analysis (FBA). In this case, the rate-of-change of metabolites is zero, while dynamical models allow to obtain the time-course of metabolites through integrating ODEs. Then what should be the stand of a hybrid model towards the steady-state assumption? The allopurinol combined PBPK and genome-scale liver model \cite{krauss2012integrating} addressed this issue through maintaining the steady-state assumption and performing the coupling on the imbalanced reactions. The imbalanced reactions in a genome-scale model are exchange or biomass reactions whose rate-of-change is not equal to zero. As a matter of fact, the steady-state assumption in genome-scale models is only partial and includes only internal metabolites. In the hybrid model of E.coli \cite{covert2008integrating}, the authors lifted the steady-state assumption on internal metabolites whose rate-of-change is known by the dynamical model and kept the remaining metabolites as steady-state. Finally, the co-existence of steady-state and non-steady metabolites is mainly driven by the availability of data and has to be clearly communicated in the modeling phase as it could lead to confusion and misinterpretation.\\
How much should be the length of the time step between discrete and continuous dynamics? Most attempts so far have taken into account the integration tolerance as a main objective \cite{covert2008integrating,gomez2014dfbalab,hanly2011dynamic}. Then a recurrent question in field comes with regards to biological relevance: does the biological system optimize for short time steps or does it have an endpoint objective function? The former seems more concordant with experimental data although the latter is more intuitive \cite{mahadevan2002dynamic}.
\section{Tractability and model reduction}
Building a hybrid body-wide model of carbohydrate metabolism and regulation (Ben Guebila and Thiele, in preparation) posed a totally new set of questions that were triggered by the exceptional size of the model (> 80,000 reactions, > 10,000 ODEs). Maintaining a chronologically coherent set of predictions required the design of new tools (\hyperref[ch:chapter5]{Chapter 5}). Particularly, the size of the model made it largely under-determined, which required the selection of the solutions that are closer to the previous time step in a flip book analogy. This type of issues was not affecting small size model as the solution space is much reduced and usually the constraints from the dynamical model ensure the smoothness of the system. The downside of this addition was that it turned the simulations into time-critical operations. The simulation is in the order of days, which affects the development and re-use of this type of models as specific hardware is required.\\
Although this question came up newly in the field of metabolic modelling, I find that getting inspiration from aerospace modelling could be salutary. In the beginning of the race to space, the large simulation time of the shuttle trajectory reportedly delayed several missions \cite{phillips2005journey}.
Model reduction is based on a simple concept: first the building of the detailed mechanistic model and the simulation for a set of output variables. Although, the original model is rich in information and the length of simulation time makes it impracticable for real world needs e.g., space shuttle launch, clinical trials, as no one would wait for months to make a go/no-go decision. The problem is often referred to as the curse of parametrisation and is often dealt with through selecting an output variable of interest to the question. Second, model order reduction is performed through projecting it into a lower dimension after determining the main readout variable of the model \cite{zahr2017multilevel,amsallem2016real}. In its compact form the model can hold on a USB thumb-drive and can be exchanged and manipulated very fast to answer what-if type of questions.
The original rich system is usually sensitive to the initial conditions, it is then perturbed and sub-regions of solutions are identified, sampled, compressed and finally clustered to represent a set of output solutions \cite{balajewicz2016projection,farhat2016recent}. This means that for a small number of large-scale simulations, the behaviour of the system can be reduced to the informative regions.
%The design of stealth aircraft uses reduced models Petr Ufimtsev
%Model form incertainty.vs parametric uncertainity
\section{A regulatory perspective}
The increasing use of hybrid models in biomedical applications and particularly in pharmacometrics naturally brings a reflection on the regulatory aspect of modeling. The FDA has been accepting PKPD and PBPK models \cite{us2004innovation} to support labelling of products, the optimal design of clinical trials, drug-drug interactions, prediction of exposure in paediatric population, estimation of absorption, and as a support for regulatory review in general \cite{sager2015physiologically,pan2014application}. Recently, a closed-loop controlled insulin pump in T1D has been accepted without preclinical trials, using solely the model simulations \cite{kovatchev2009silico}. What would be the stand of the regulator towards hybrid models of continuous and discrete dynamics?\\
While PKPD models are the smallest models that best fit the data, this type of hybrid model is certainly not minimal. Constraint-based modeling is rather a semi-quantitative approach and is not as accurate as full dynamical models in predicting reaction rates. As such, the hybrid model is semi-quantitative, particularly for the metabolites and reactions that are covered by the genome-scale metabolic model alone. Hybrid models are certainly useful in preclinical trials, target discovery, and biomarker identification. In clinical trials, a large set of constraints are needed to constrain the hybrid model to the biologically relevant observations, such as the constraints provided by the GIM model that covers all the organs and a large number of processes. Nevertheless, the detail and scope of the GIM model make it certainly anecdotal, there are a handful of models that are equal in granularity and size. Particularly, to estimate the large number of kinetic parameters in PBPK models, concentration data in the target organs need to be measured to accurately estimate organ-specific rates of metabolism. Nevertheless, the use of organ diffusion models can provide an accurate estimation of the metabolised fraction \cite{rodgers2005physiologically,rodgers2006physiologically}. In order to perform classical pharamacometric analyses on large-scale models including bootstrapping of parameters, finding correlation between parameters, and mixed-effect non-linear population modeling, new mathematical tools have to be developed in order to meet the needs of the regulatory bodies, particularly through addressing parametric uncertainty and model-form uncertainty.\\
The increasingly accessible biological data combined with adequate open-source software such as PKSIM \cite{willmann2003pk}, will make it easier to build whole-body models, which can bring modeling in the forefront of clinical trials design.
\section{What is next? writing the ODE of life.}
Many view hybrid models of continuous and discrete dynamics as a a pitstop towards full dynamical models of biological systems. Recently, a few large-scale ODE systems of bacterial metabolism were developed \cite{khodayari2016genome,dash2017development}, equalling in size the first generation of constraint-based metabolic models. Dynamical models are certainly more adequate than constraint-based models in quantitatively answering specific questions with respect to the system dynamics, particularly when the uncertainty is quantified and assessed. Although seeing them as the ultimate goal of explaining life is certainly delusional. This is because of the large number of parameters to estimate, the different rate processes, the modeling of the different types of affinities and inhibitory processes,  and the \textit{in vivo} enzyme kinetics. While I think this goal is within reach in the upcoming years, I believe that smaller tractable models can provide an equally insightful result to a specific question at a given time and for a fraction of the development cost; to quote Von Neumann: 'If people do not believe that mathematics is simple, it is only because they do  not realize how complicated life is'.